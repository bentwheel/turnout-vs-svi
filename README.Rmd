---
title: "Modeling County-Level Voter Turnout Using the CDC's Social Vulnerability Index (SVI)"
author: "C. Seth Lester, ASA, MAAA"
date: "2024-11-01"
output: github_document
always_allow_html: true
---

```{r setup, message=F, warning=F, include=F}
library(tidyverse)
library(ggpubr)
library(ggrepel)
library(broom)
library(ggcorrplot)
library(caret)
library(glmnet)

source(here::here("./src/get_svi.R"))
source(here::here("./src/get_vep_totals.R"))
source(here::here("./src/get_election_data.R"))

```

# Background

This project was inspired by insights shared in a recent LinkedIn post where I explored whether the CDC’s [Social Vulnerability Index](https://www.atsdr.cdc.gov/placeandhealth/svi/index.html) (SVI) could accurately predict county-level voter turnout in US Presidential elections. 

Public presidential polls often use likely voter models to re-weight raw polling samples, incorporating demographic factors such as race/ethnicity, poverty level, and education level, which overlap significantly with components of the SVI. In healthcare analytics, the SVI is commonly used to account and control for geographic variations in [social determinants of health](https://odphp.health.gov/healthypeople/priority-areas/social-determinants-health) that can influence or confound causal relationships between healthcare interventions and consequent cost/utilization patterns for a population.

Motivated by [Yubin Park’s concept of blending unrelated datasets to create “scientific curries,”](https://www.linkedin.com/posts/yubin-park-phd_does-the-use-of-unusual-combinations-of-datasets-activity-7249938421907865600-oXwk/?utm_source=share&utm_medium=member_desktop) I set out to investigate how social vulnerabilities might impact civic engagement - particularly through the lens of examining the relationship between social vulnerability and voter turnout at the county level of granularity.

# Data Sources and Methods

The analysis relies on three primary data sources:

## American Community Survey (ACS)

This data is accessed using a [US Census Bureau API](https://api.census.gov/data/key_signup.html) key and the ```tidycensus``` R package. This data provides demographic and population estimates for counties, which are integral in this analysis for determining county-level estimates of the Voting Age Population (VAP) and Voting Eligible Population (VEP). This work of staging this data is done in the R script ```src/get_vep_totals.R```. 

The VAP is calculated by determining the number of individuals age 18 and over in a particular county using 5-year ACS data. VEP is calculated by subtracting the count of non-citizen individuals aged 18 and over from VAP. While these counts might slightly overestimate true VAP/VEP due to the inclusion of certain ineligible groups (e.g., felons in some states), they provide a consistent and reliable set of denominators for county-level turnout analysis.

As there is generally a 1-2 year lag between the ACS measures for a time period and the time these measures are compiled and released by the US Census Bureau, this project uses VAP/VEP totals at the county level based on the ACS data for the five year period ending in YYYY - 2 for any presidential election year in YYYY. For example, the VEP/VAP measures used to calculate turnout rates at the county level for the 2016 election are determined using 5-year ACS measures from 2010 - 2014.

Ultimately, our goal is to devise a prediction model for turnout using ACS / SVI measures that tend to have a 1-2 year lag in availability. Ultimately, if we want to build a turnout model to predict 2024 election turnout, we will need to use 2018 - 2022 5-year ACS measures, as that will be the latest data available for constructing SVI measures.

## MIT Election Lab 

[MIT Election Lab](https://electionlab.mit.edu/data) data contains historical county-level election returns, including data from 2012, 2016, and 2020. This data allows for a comprehensive analysis of turnout trends over multiple election cycles. This data is staged in the R script ```src/get_election_data.R```.

This data is relatively straightforward, with one record per county that is joinable to the VEP/VAP data gathered from ACS and the SVI factors gathered from the CDC. One issue in this data is tabulations for county votes in Alaska. Alaska is uncommon in that the entirety of the state is not subdivided into counties - some people live outside of counties (Boroughs) - so for the sake of analyzing the relationship between SVI and turnout, I thought it best to remove Alaska vote data. We're removing a very small piece of the sample. Sorry Alaska! 

**Caveat**: *don't* use this turnout model to predict Alaska results!

## CDC Social Vulnerability Index (SVI)

The SVI is produced every 2-4 years by the CDC based on measures contained in the 5-year ACS data. This freely-downloadable dataset contains an overall SVI score for counties / census tracts in the US. The overall SVI score is further distilled from four component scores that measure social vulnerability on four categories: socioeconomic status, household composition, racial/ethnic minority status, and housing type/transportation.

![Image of Captain Planet's Planeteers summonning the various components of the SVI.](./svi_planeteers.jpg)

These scores (and their component pieces) are used subsequently in this analysis to quantify impact of social factors on turnout rates in Presidential elections. The SVI data required for this analysis is loaded and staged in the R script ```src/get_svi.R```.

The process used by CDC/ATSDR to calculate SVI with the underlying ACS 5-year measures underwent a large number of changes in 2020. In order to evaluate the concern that SVI (or its components) are not sufficiently stable over time, I visually evaluated the SVI (and four underlying component measures) over time for the top 25 largest (by population) US counties to check that the SVI redesign in 2020 did not led to substantial volatility in the measure (or, at least, moreso than the actual year 2020 would have added to the measure).

### 2010 - 2022 SVI for Top 25 Most Populous US Counties

To better understand variation over time in the SVI measures, I examined the history of variation in the overall SVI measure for the top 25 counties in the US (ranked by population size).

First, we start with overall SVI, which ranks each county in the US by percentile of overall social vulnerability based on the component sum of the four social vulnerability themes:

* Socioeconomic Status
* Household Characteristics
* Racial & Ethnic Minority Status
* Housing Type & Transportation

The overall SVI for each county is determined by adding the sum of each of the percentile rankings of these four themes together for each particular county, and then percentile-ranking the overall sum for each county. It's important to note that no one factor is given larger "weight" than another in this calculation, which makes the computation of SVI quite simple - but also might leave something to be desired in terms of *accurately measuring social vulnerability*.

```{r svi-top-25-over-time, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=10}

# Calculate average population for each county across all years
top_25_counties <- svi_all %>%
  group_by(GEOID, LOCATION) %>%
  summarize(avg_population = mean(E_TOTPOP, na.rm = TRUE)) %>%
  arrange(desc(avg_population)) %>%
  ungroup() %>% 
  slice_head(n = 25)

# Filter SVI data for only the top 20 counties
svi_filtered <- svi_all %>%
  inner_join(top_25_counties, by = c("GEOID", "LOCATION")) %>% 
    mutate(dateyr = as.Date(ymd(paste0(year, "0101"))))

# Create the plot
svi_top25_over_time.plot <- svi_filtered %>%
  arrange(LOCATION, year) %>% 
  ggplot(aes(x = dateyr, y = RPL_THEMES, color = LOCATION, group = LOCATION)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_label_repel(
    aes(label = LOCATION), 
    size = 2.5, 
    force = 2,
    min.segment.length = 0,  # Allow long segments
    nudge_x = 1000,  # Nudge labels to the right
    xlim = c(as.Date("2022-01-01"), as.Date("2027-01-01")),  # Extend label space
    data = svi_filtered %>% filter(year == max(year)),
    direction = "y",  # Keep labels vertically aligned
    hjust = 0,
    max.iter = 100000,
    show.legend = FALSE
  ) +
  scale_color_manual(values = scales::hue_pal()(25)) + 
  scale_x_date(
    limits = as.Date(c("2009-01-01", "2027-12-31")),  # Limit the date range
    breaks = as.Date(c("2010-01-01", "2014-01-01", "2016-01-01",
                    "2018-01-01", "2020-01-01", "2022-01-01")),  
    date_labels = "%Y"  # Format as year only
  ) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "Social Vulnerability Index (SVI) Trends",
    subtitle = "Top 25 US Counties by Population (2010-2022)",
    x = "Year",
    y = "Percentile Social Vulneability Score",
    color = "County",
    caption = "Source: CDC/ATSDR Social Vulnerability Index (SVI)\nhttps://www.atsdr.cdc.gov/placeandhealth/svi/index.html"
  ) +
  theme_bw() +
  theme(
    legend.position = "none",  # no legend
    plot.title = element_text(size = 16),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  ) 

# Display the plot
svi_top25_over_time.plot

```

Next, I examined the four component SVI themes over time, separately.

#### 2010 - 2022 Socioeconomic Status Component for Top 25 Most Populous US Counties

```{r theme1-top-25-over-time, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=10}

# Create the plot
theme1_top25_over_time.plot <- svi_filtered %>%
  arrange(LOCATION, year) %>% 
  ggplot(aes(x = dateyr, y = RPL_THEME1, color = LOCATION, group = LOCATION)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_label_repel(
    aes(label = LOCATION), 
    size = 2.5, 
    force = 2,
    min.segment.length = 0,  # Allow long segments
    nudge_x = 1000,  # Nudge labels to the right
    xlim = c(as.Date("2022-01-01"), as.Date("2027-01-01")),  # Extend label space
    data = svi_filtered %>% filter(year == max(year)),
    direction = "y",  # Keep labels vertically aligned
    hjust = 0,
    max.iter = 100000,
    show.legend = FALSE
  ) +
  scale_color_manual(values = scales::hue_pal()(25)) + 
  scale_x_date(
    limits = as.Date(c("2009-01-01", "2027-12-31")),  # Limit the date range
    breaks = as.Date(c("2010-01-01", "2014-01-01", "2016-01-01",
                    "2018-01-01", "2020-01-01", "2022-01-01")),  
    date_labels = "%Y"  # Format as year only
  ) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "Socioeconomic Status Component of SVI Trends",
    subtitle = "Top 25 US Counties by Population (2010-2022)",
    x = "Year",
    y = "Percentile Social Vulneability Score",
    color = "County",
    caption = "Source: CDC/ATSDR Social Vulnerability Index (SVI)\nhttps://www.atsdr.cdc.gov/placeandhealth/svi/index.html"
  ) +
  theme_bw() +
  theme(
    legend.position = "none",  # no legend
    plot.title = element_text(size = 16),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  ) 

# Display the plot
theme1_top25_over_time.plot

```

#### 2010 - 2022 Household Characteristics Component for Top 25 Most Populous US Counties

```{r theme2-top-25-over-time, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=10}

# Create the plot
theme2_top25_over_time.plot <- svi_filtered %>%
  arrange(LOCATION, year) %>% 
  ggplot(aes(x = dateyr, y = RPL_THEME2, color = LOCATION, group = LOCATION)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_label_repel(
    aes(label = LOCATION), 
    size = 2.5, 
    force = 2,
    min.segment.length = 0,  # Allow long segments
    nudge_x = 1000,  # Nudge labels to the right
    xlim = c(as.Date("2022-01-01"), as.Date("2027-01-01")),  # Extend label space
    data = svi_filtered %>% filter(year == max(year)),
    direction = "y",  # Keep labels vertically aligned
    hjust = 0,
    max.iter = 100000,
    show.legend = FALSE
  ) +
  scale_color_manual(values = scales::hue_pal()(25)) + 
  scale_x_date(
    limits = as.Date(c("2009-01-01", "2027-12-31")),  # Limit the date range
    breaks = as.Date(c("2010-01-01", "2014-01-01", "2016-01-01",
                    "2018-01-01", "2020-01-01", "2022-01-01")),  
    date_labels = "%Y"  # Format as year only
  ) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "Household Characteristics Component of SVI Trends",
    subtitle = "Top 25 US Counties by Population (2010-2022)",
    x = "Year",
    y = "Percentile Social Vulneability Score",
    color = "County",
    caption = "Source: CDC/ATSDR Social Vulnerability Index (SVI)\nhttps://www.atsdr.cdc.gov/placeandhealth/svi/index.html"
  ) +
  theme_bw() +
  theme(
    legend.position = "none",  # no legend
    plot.title = element_text(size = 16),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  ) 

# Display the plot
theme2_top25_over_time.plot

```

#### 2010 - 2022 Racial & Ethnic Minority Status Component for Top 25 Most Populous US Counties

```{r theme3-top-25-over-time, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=10}

# Create the plot
theme3_top25_over_time.plot <- svi_filtered %>%
  arrange(LOCATION, year) %>% 
  ggplot(aes(x = dateyr, y = RPL_THEME3, color = LOCATION, group = LOCATION)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_label_repel(
    aes(label = LOCATION), 
    size = 2.5, 
    force = 2,
    min.segment.length = 0,  # Allow long segments
    nudge_x = 1000,  # Nudge labels to the right
    xlim = c(as.Date("2022-01-01"), as.Date("2027-01-01")),  # Extend label space
    data = svi_filtered %>% filter(year == max(year)),
    direction = "y",  # Keep labels vertically aligned
    hjust = 0,
    max.iter = 100000,
    show.legend = FALSE
  ) +
  scale_color_manual(values = scales::hue_pal()(25)) + 
  scale_x_date(
    limits = as.Date(c("2009-01-01", "2027-12-31")),  # Limit the date range
    breaks = as.Date(c("2010-01-01", "2014-01-01", "2016-01-01",
                    "2018-01-01", "2020-01-01", "2022-01-01")),  
    date_labels = "%Y"  # Format as year only
  ) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "Racial & Ethnic Minority Status Component of SVI Trends",
    subtitle = "Top 25 US Counties by Population (2010-2022)",
    x = "Year",
    y = "Percentile Social Vulneability Score",
    color = "County",
    caption = "Source: CDC/ATSDR Social Vulnerability Index (SVI)\nhttps://www.atsdr.cdc.gov/placeandhealth/svi/index.html"
  ) +
  theme_bw() +
  theme(
    legend.position = "none",  # no legend
    plot.title = element_text(size = 16),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  ) 

# Display the plot
theme3_top25_over_time.plot

```

#### 2010 - 2022 Housing Type & Transportation Component for Top 25 Most Populous US Counties

```{r theme4-top-25-over-time, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=10}

# Create the plot
theme4_top25_over_time.plot <- svi_filtered %>%
  arrange(LOCATION, year) %>% 
  ggplot(aes(x = dateyr, y = RPL_THEME4, color = LOCATION, group = LOCATION)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_label_repel(
    aes(label = LOCATION), 
    size = 2.5, 
    force = 2,
    min.segment.length = 0,  # Allow long segments
    nudge_x = 1000,  # Nudge labels to the right
    xlim = c(as.Date("2022-01-01"), as.Date("2027-01-01")),  # Extend label space
    data = svi_filtered %>% filter(year == max(year)),
    direction = "y",  # Keep labels vertically aligned
    hjust = 0,
    max.iter = 100000,
    show.legend = FALSE
  ) +
  scale_color_manual(values = scales::hue_pal()(25)) + 
  scale_x_date(
    limits = as.Date(c("2009-01-01", "2027-12-31")),  # Limit the date range
    breaks = as.Date(c("2010-01-01", "2014-01-01", "2016-01-01",
                    "2018-01-01", "2020-01-01", "2022-01-01")),  
    date_labels = "%Y"  # Format as year only
  ) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "Housing Type & Transportation Component of SVI Trends",
    subtitle = "Top 25 US Counties by Population (2010-2022)",
    x = "Year",
    y = "Percentile Social Vulneability Score",
    color = "County",
    caption = "Source: CDC/ATSDR Social Vulnerability Index (SVI)\nhttps://www.atsdr.cdc.gov/placeandhealth/svi/index.html"
  ) +
  theme_bw() +
  theme(
    legend.position = "none",  # no legend
    plot.title = element_text(size = 16),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  ) 

# Display the plot
theme4_top25_over_time.plot

```

With the possible exception of the third component of SVI (the Racial & Ethnic Minority Status component), the four SVI components appear to be relatively stable over time when considering that in 2020 we could expect a considerable amount of variation in how SVI was measured due to both pandemic-related factors as well as underlying bias in ACS measurements sampled in 2020. Another possible explanation for the instability of the third component could be that in 2020 this component was heavily redefined. 

When considering our intended task of building a turnout model with the SVI, we might consider using the underlying variables for this cateogory instead of the component percentile rank variables, as those are more stable over time.


# Initial Examination

The goal of this project is to eventually use election returns turnout data for 2012, 2016, and 2020 to develop a turnout model that uses SVI data from the prior 2 years to predict turnout at the county level.

Prior to beginning this modeling exercise I wanted to better understand the distribution of our response - presidential election turnout rates at the county level.

## A first pass examining the distribution of turnout

First we want to understand the distribution of turnout using both our denominators (VAP and VEP). Note that sometimes (in 11 cases) the total number of votes received in a county will equal or exceed estimates of VAP. This is typically due to counties that are VERY small with regard to population, so the ACS estimate of population is likely to be an undercount. 

The 11 non-Alaska counties where this occurs are very sparsely-populated rural counties with merely hundreds (at most, 2,416) residents, and were won by both Democratic and Republican candidates. There is NOT sufficient data precision in ACS population estimates to prove anything about unauthorized people voting, so put those silly tinfoil hats away, please.

First, we want to join our election data (from one source) to our ACS 5-year population estimates for VEP and VAP (from another source). We will join on FIPS code and then calculate turnout rates using both VAP and VEP as denominators. Then we will analyze the distribution of both turnout measures and remove any outliers (perhaps due to very high turnout in very low-population areas). 

We define an outlier for both VAP- and VEP-based turnout rates as any county for any election year with a turnout rate that exceeds 3 standard deviations above the mean of turnout. This process removes the 11 counties mentioned above with a VAP/VEP greater than 100%, but plus an additional 21 (extremely low population) counties with a turnout rate that is likely overestimated due to an undercounted denominator. With over 3,000 counties in our sample for each election year, we're losing very little sample space by doing this!

```{r election-vep-data-join-analysis, message=F, warning=F, include=T, echo=F, fig.height=6, fig.width=10}

election_data_w_vep <- election_data %>% 
  filter(state_po != "AK") %>% 
  left_join(us_vep_data) %>% 
  mutate(turnout_vap = totalvotes / VAP_denom,
         turnout_vep = totalvotes / VEP_denom) %>% 
  filter(!is.na(turnout_vap))

# Determine outliers (outside of 3 standard deviations of mean turnout)
outlier_thresholds <- election_data_w_vep %>% 
  group_by(year) %>% 
  summarize(turnout_vap_mean = mean(turnout_vap),
            turnout_vap_sd = sd(turnout_vap),
            turnout_vep_mean = mean(turnout_vep),
            turnout_vep_sd = sd(turnout_vep)) %>% 
  ungroup() %>% 
  mutate(vap_lo = turnout_vap_mean - 3*turnout_vap_sd,
         vap_hi = turnout_vap_mean + 3*turnout_vap_sd,
         vep_lo = turnout_vep_mean - 3*turnout_vep_sd,
         vep_hi = turnout_vep_mean + 3*turnout_vep_sd)

# Filter / remove outlier counties
election_data_w_vep_no_out <- election_data_w_vep %>% 
  left_join(outlier_thresholds) %>% 
  filter(turnout_vep >= vep_lo & turnout_vep <= vep_hi)

# Precompute means for each year and party combination
means_data <- election_data_w_vep_no_out %>%
  filter(year %in% c(2012, 2016, 2020)) %>%
  mutate(party_lbl = if_else(winning_party == "DEMOCRAT", "Counties won by Dem Party", "Counties won by GOP")) %>%
  group_by(year, party_lbl) %>%
  summarize(mean_vep = mean(turnout_vep), .groups = "drop")

# Display VAP/VEP turnout with mean lines and labels in the corner of each panel
turnout_distr.plot <- election_data_w_vep_no_out %>%
  filter(year %in% c(2012, 2016, 2020)) %>% 
  select(year, state, state_po, county_name, county_fips, winning_party, turnout_vep) %>%
  mutate(party_lbl = if_else(winning_party == "DEMOCRAT", "Counties won by Dem Party", "Counties won by GOP")) %>%
  ggplot(aes(x = turnout_vep, fill = winning_party)) + 
  scale_fill_manual(values = c("blue", "red")) + 
  scale_x_continuous(labels = scales::label_percent(accuracy = 1)) +
  geom_histogram(bins = 40, color = "gray20") +
  geom_vline(data = means_data, aes(xintercept = mean_vep, color=winning_party), linetype = "dashed", color = "black") +  # Add dashed line for mean
  geom_label(
    data = means_data,
    aes(
      x = Inf,  # Place labels near the top right, adjust as needed
      y = Inf,
      label = paste("Mean:", scales::percent(mean_vep, accuracy = 0.1))
    ),
    hjust = 1.1, vjust = 1.1,
    size = 3,
    color = "black",
    fill = "white",
    label.size = 0
  ) +
  facet_grid(year ~ party_lbl) + 
  labs(
    title = "Distribution of County-Level Turnout as Percent of Voting-Eligible Population (VEP)",
    subtitle = "By Presidential Election Year and Winning Party",
    x = "Turnout Rate (Percentage of VEP)",
    y = "Number of Counties"
  ) + 
  theme_bw() + 
  theme(legend.position = "none")

turnout_distr.plot

```

## Turnout over time

Since I intend to use data from 2012, 2016, and 2020 presidential elections to build the turnout model, I thought it best to next examine election turnout rates over time for the top 25 counties, as I did earlier with SVI and its four component themes.

```{r top-25-turnout-over-time, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=10}

# Create the plot
turnout_top25_over_time.data <- top_25_counties %>% 
  mutate(county_fips = as.numeric(GEOID)) %>% 
  select(county_fips, LOCATION) %>% 
  left_join(election_data_w_vep_no_out) %>% 
  filter(year %in% c(2012, 2016, 2020)) %>%
  mutate(dateyr = ymd(paste0(year, "0101"))) 

turnout_top25_over_time.plot <- turnout_top25_over_time.data %>% 
  ggplot(aes(x = dateyr, y = turnout_vep, color = LOCATION, group = LOCATION)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_label_repel(
    aes(label = LOCATION), 
    size = 2.5, 
    force = 2,
    min.segment.length = 0,  # Allow long segments
    nudge_x = 1000,  # Nudge labels to the right
    xlim = c(as.Date("2020-01-01"), as.Date("2025-01-01")),  # Extend label space
    data = turnout_top25_over_time.data %>% filter(year == max(year)),
    direction = "y",  # Keep labels vertically aligned
    hjust = 0,
    max.iter = 100000,
    show.legend = FALSE
  ) +
  scale_color_manual(values = scales::hue_pal()(25)) + 
  scale_x_date(
    limits = as.Date(c("2009-01-01", "2026-12-31")),  # Limit the date range
    breaks = as.Date(c("2012-01-01", "2016-01-01", "2020-01-01")),  
    date_labels = "%Y"  # Format as year only
  ) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "Presidential Election Turnout (% of VEP) Trends",
    subtitle = "Top 25 US Counties by Population (2012-2020)",
    x = "Year",
    y = "Turnout % of Voting Eligible Population (VEP)",
    color = "County",
    caption = "Source: MIT Election Labs \nhttps://electionlab.mit.edu/data"
  ) +
  theme_bw() +
  theme(
    legend.position = "none",  # no legend
    plot.title = element_text(size = 16),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  ) 

# Display the plot
turnout_top25_over_time.plot

```


## Turnout vs. SVI in Presidential Elections

Finally, having combined turnout rates and corresponding 2-year-lagged SVI data for the three presidential election years, I plotted the relationship between overall SVI at the county level and county-level turnout rates. 

```{r initial-examination, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=13}

# SVI vs. Turnout
svi_vs_turnout.data <- svi_all %>% 
  filter(RPL_THEMES >= 0) %>% 
  filter(year %in% c(2010, 2014, 2018)) %>% 
  mutate(year = year + 2) %>% 
  select(ST, ST_ABBR, GEOID, LOCATION, RPL_THEME1, RPL_THEME2, RPL_THEME3, RPL_THEME4, RPL_THEMES, year) %>% 
  inner_join(election_data_w_vep_no_out %>% select(GEOID, year, winning_party, VEP_denom, turnout_vep)) %>% 
  mutate(vote_winner = if_else(winning_party == "REPUBLICAN", "GOP won", "Dem won"),
         facet_lbl = paste0(year-2, " SVI vs. ", year, "\nPresidential Election Turnout"))

svi_vs_turnout.plot <- svi_vs_turnout.data %>% 
  ggplot(aes(x = RPL_THEMES, y = turnout_vep, fill=vote_winner)) +
  geom_point(aes(size = VEP_denom), alpha = .6, shape=21) +
  geom_smooth(aes(group = 1), method = "lm", color = "black", se = FALSE, show.legend = F) +  # Add best-fit line
  facet_wrap(facet_lbl~., nrow=1) +
  scale_fill_manual(values = c("blue", "red")) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  scale_x_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "Comparing CDC Social Vulnerability Index (SVI) Scores\nwith Historical Voter Turnout at the County Level",
    subtitle = "By Election Year",
    x = "SVI Score",
    y = "Total Votes as % of Estimated Citizen Population from 5-yr ACS",
    fill = ""
  ) +
  theme_bw() +
  guides(size = "none") +
  stat_cor(
    aes(group = 1, label = after_stat(r.label)),
    method = "pearson",
    label.x.npc = "middle",  # Position label at middle of panel
    label.y.npc = "top",   # Position label at top of panel
    size = 4,
    color = "black"
  ) +
  theme(legend.position = "bottom",
        plot.title = element_text(size = rel(1.2)),
        strip.text = element_text(size = rel(1.3)),
        legend.text = element_text(size = rel(1.2)))

svi_vs_turnout.plot


```

The image above suggests a relatively moderate negative Pearson correlation (R) between SVI and turnout rates. I thought this was pretty darn interesting, so I posted a version of this chart on [LinkedIn](https://www.linkedin.com/posts/csethlester_can-the-cdcs-social-vulnerability-index-activity-7259468140282019841-7ue4?utm_source=share&utm_medium=member_desktop).

I then examined this relationship further by examining the correlation at the 4 primary SVI component measures as compared to turnout rates, where I also found remarkable stability within each SVI component measure over the three presidential elections.

```{r themes-vs-turnout, message=F, warning=F, include=T, echo=F, fig.height=10, fig.width=13}

# SVI vs. Themes

themes_vs_turnout.data <- svi_vs_turnout.data %>% 
  select(GEOID, year, LOCATION, vote_winner, facet_lbl, VEP_denom, turnout_vep, starts_with("RPL_THEME")) %>% 
  select(-RPL_THEMES) %>% 
  mutate(year = as.character(year)) %>% 
  pivot_longer(cols = -c(GEOID, year, LOCATION, vote_winner, facet_lbl, VEP_denom, turnout_vep),
               names_to = "theme_no",
               values_to = "pct_rank",
               names_pattern = "^RPL_THEME(1|2|3|4)$") %>% 
  mutate(svi_theme = case_when(
    theme_no == 1 ~ "Socioeconomic\nStatus",
    theme_no == 2 ~ "Household\nCharacteristics",
    theme_no == 3 ~ "Racial/Ethnic\nMinority Status",
    theme_no == 4 ~ "Housing Type and\nTransportation")) %>% 
  arrange(theme_no) %>% 
  mutate(svi_theme = fct_inorder(svi_theme))


themes_vs_turnout.plot <- themes_vs_turnout.data %>% 
  ggplot(aes(x = pct_rank, y = turnout_vep, fill=vote_winner)) +
  geom_point(aes(size = VEP_denom), alpha = .6, shape=21) +
  geom_smooth(aes(group = 1), method = "lm", color = "black", se = FALSE, show.legend = F) +  # Add best-fit line
  facet_grid(svi_theme~facet_lbl) +
  scale_fill_manual(values = c("blue", "red")) +
  scale_y_continuous(labels = scales::label_percent(accuracy=1)) +
  scale_x_continuous(labels = scales::label_percent(accuracy=1)) +
  labs(
    title = "SVI Component Scores vs. Historical Voter Turnout at the County Level",
    subtitle = "By Election Year",
    x = "Percentile Rank of SVI Component",
    y = "Total Votes as % of Estimated Citizen Population from 5-yr ACS",
    fill = ""
  ) +
  theme_bw() +
  guides(size = "none") +
  stat_cor(
    aes(group = 1, label = after_stat(r.label)),
    method = "pearson",
    label.x.npc = "middle",  # Position label at middle of panel
    label.y.npc = "top",   # Position label at top of panel
    size = 4,
    color = "black"
  ) +
  theme(legend.position = "bottom",
        plot.title = element_text(size = rel(1.2)),
        strip.text = element_text(size = rel(1.3)),
        legend.text = element_text(size = rel(1.2)))

themes_vs_turnout.plot


```

Just from a cursory scan at Pearson correlation broken out by the four major SVI components, it looks like the real breadwinner varibles for a potential turnout model will come from the Socioeconomic Status and Housing Type/Transportation categories, with perhaps some useful information in the Household Characteristics category. It seems unintuitive that racial/ethnic identity would have a meaningful relationship with turnout rates for a county, but we'll also consider component features from this SVI category as we build our prediction model.


# Let's Build a Turnout Model!

OK, enough description of the data - let's use SVI to predict the future!

## What are we modeling?

We want a model that will predict turnout for election year YYYY based on the SVI file for YYYY - 2. Also, it is my hope that our model will predict not *absolute* turnout (that is, number of votes in each county), but rather, a US county's *relative* turnout expressed as a multiplicative factor of the state-level turnout. 

Since different states will have different degrees of turnout relative to other factors (e.g., ad spending, swing state status, etc.), the model is intended to be used as a means to predict turnout relativities between counties in a particular state. I will demonstrate usage of the model in a final section to predict turnout in my home state of North Carolina in the 2024 election.

(Also, if you're following along at home, I designed this model like a risk adjustment model.)

In short, we will train our model on a response that is the value of turnout multiplicitavely scaled by dividing the turnout rate variable for each county by its own average across all counties for each election year. Then, when our model predicts a turnout score of 1.00 +/- x, we will interpret that to mean that the county is expected to have a turnout equal to 1.00 +/- x times the average turnout across all counties for that election (represented by 1.00).

## Should we include 2020 data?

Furthermore, 2020 presented some unique challenges (and unique motivations) that might not reflect more stable patterns reflecting the general relationship between SVI variables and voter turnout. 

Before writing all this up, I examined the performance between two models - one trained including 2020 data, and one trained without 2020 data - and found no material difference in predictive performance. 

Therefore, 2020 data is included.

## Model Design and Feature Selection

### Inaugural Feature Set

Ideally we would want a model that is trained on pre-2024 returns data, but predictive of turnout outcomes in 2024. The variables used to measure overall SVI across all four categories which are present across **all versions** of the SVI data available (2010 - 2022) are as follows:

| SVI Feature   | Description                                                                               |
|---------------|-------------------------------------------------------------------------------------------|
| EPL_POV*      | Percentile percentage of persons below 100% (2010 - 2020) / 150% (2020+) poverty estimate |
| EPL_UNEMP     | Percentile percentage of civilian (age 16+) unemployed estimate                           |
| EPL_NOHSDP    | Percentile percentage of persons with no high school diploma (age 25+) estimate           |
| EPL_AGE65     | Percentile percentage of persons aged 65 and older estimate                               |
| EPL_AGE17     | Percentile percentage of persons aged 17 and younger estimate                             | 
| EPL_SNGPNT    | Percentile percentage of single-parent households with children under 18 estimate         |
| EPL_LIMENG    | Percentile percentage of persons (age 5+) who speak English "less than well"              |
| EPL_MINRTY    | Percentile percentage of persons who identify as a racial/ethnic identity in the minority |
| EPL_MUNIT     | Percentile percentage housing in structures with 10 or more units                         |
| EPL_MOBILE    | Percentile percentage mobile homes                                                        |
| EPL_CROWD     | Percentile percentage households with more than 1 person per room                         |
| EPL_NOVEH     | Percentile percentage households with no vehicle available                                |
| EPL_GROUPQ    | Percentile percentage of persons in group quarters estimate                               |

Often a very good *first step* in building a predictive model is to get a handle on your feature space - including understanding their distribution. Since the EPL_* varaibles are percentile ranks, we can expect that these variables are all likely expressive of a uniform distribution across a support of 0 to 100%. Let's confirm that now:

```{r feature-distr-plot, echo=F, include=T, warning=F, error=F, fig.height=8, fig.width=8}

model_data <- svi_all %>% 
  filter(RPL_THEMES >= 0) %>% 
  filter(year %in% c(2010, 2014, 2018)) %>% 
  mutate(year = year + 2) %>% 
  inner_join(election_data_w_vep_no_out %>% select(GEOID, year, winning_party, VEP_denom, turnout_vep)) %>% 
  group_by(year, ST_ABBR) %>% 
  mutate(turnout_bystate_total = sum(turnout_vep*VEP_denom) / sum(VEP_denom)) %>% 
  ungroup() %>% 
  mutate(scaled_turnout = turnout_vep / turnout_bystate_total) 

epl_data <- model_data %>%
  select(EPL_POV, EPL_UNEMP, EPL_NOHSDP, EPL_AGE65, EPL_AGE17, 
         EPL_SNGPNT, EPL_LIMENG, EPL_MINRTY, EPL_MUNIT, 
         EPL_MOBILE, EPL_CROWD, EPL_NOVEH, EPL_GROUPQ) %>%
  pivot_longer(cols = everything(), names_to = "EPL_variable", values_to = "value")

# Create faceted histogram plot
ggplot(epl_data, aes(x = value)) +
  geom_histogram(bins = 25, fill = "skyblue", color = "black") +
  facet_wrap(~ EPL_variable, scales = "free") +
  labs(
    title = "Distribution of SVI EPL Variables (Percentile Rankings)",
    x = "Percentile Rank",
    y = "Frequency",
    caption = "Source: CDC/ATSDR Social Vulnerability Index (SVI)\nhttps://www.atsdr.cdc.gov/placeandhealth/svi/index.html"
  ) +
  theme_minimal()


```

We have now confirmed that every variable in the feature space reflects a Uniform(0,100) distribution. This might not be an approach that will lead to a feature space that sets us up with a high-performing predictive model, so let's investigate the non-percentile-ranked (i.e., estimates of proportions) variables underlying these within the loaded SVI datasets.

Each of these percentile ranked measurements are based on a US-wide percentile ranking of underlying proportion measures that are drawn directly from ACS 5-year data for the applicable time period. Each of these proportion measures is also present in SVI datasets.

| SVI Feature   | Description                                                                               |
|---------------|-------------------------------------------------------------------------------------------|
| EP_POV*      | Percentage of persons below 100% (2010 - 2020) / 150% (2020+) poverty estimate |
| EP_UNEMP     | Percentage of civilian (age 16+) unemployed estimate                           |
| EP_NOHSDP    | Percentage of persons with no high school diploma (age 25+) estimate           |
| EP_AGE65     | Percentage of persons aged 65 and older estimate                               |
| EP_AGE17     | Percentage of persons aged 17 and younger estimate                             | 
| EP_SNGPNT    | Percentage of single-parent households with children under 18 estimate         |
| EP_LIMENG    | Percentage of persons (age 5+) who speak English "less than well"              |
| EP_MINRTY    | Percentage of persons who identify as a racial/ethnic identity in the minority |
| EP_MUNIT     | Percentage housing in structures with 10 or more units                         |
| EP_MOBILE    | Percentage mobile homes                                                        |
| EP_CROWD     | Percentage households with more than 1 person per room                         |
| EP_NOVEH     | Percentage households with no vehicle available                                |
| EP_GROUPQ    | Percentage of persons in group quarters estimate                               |

```{r feature-distr-plot2, echo=F, include=T, warning=F, error=F, fig.height=10, fig.width=10}

model_data <- svi_all %>% 
  filter(RPL_THEMES >= 0) %>% 
  filter(year %in% c(2010, 2014, 2018)) %>% 
  mutate(year = year + 2) %>% 
  inner_join(election_data_w_vep_no_out %>% select(GEOID, year, winning_party, VEP_denom, turnout_vep)) %>% 
  group_by(year, ST_ABBR) %>% 
  mutate(turnout_bystate_total = sum(turnout_vep*VEP_denom) / sum(VEP_denom)) %>% 
  ungroup() %>% 
  mutate(scaled_turnout = turnout_vep / turnout_bystate_total) 

ep_data <- model_data %>%
  select(EP_POV, EP_UNEMP, EP_NOHSDP, EP_AGE65, EP_AGE17, 
         EP_SNGPNT, EP_LIMENG, EP_MINRTY, EP_MUNIT, 
         EP_MOBILE, EP_CROWD, EP_NOVEH, EP_GROUPQ) %>%
  filter_all(all_vars(. > 0)) %>%
  pivot_longer(cols = everything(), names_to = "EP_variable", values_to = "value")

# Create faceted histogram plot
ggplot(ep_data, aes(x = value)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "gray20") +
  facet_wrap(~ EP_variable, scales = "free") +
  labs(
    title = "Distribution of SVI EP Variables (Estimated Proportions of Total Pop.)",
    x = "Pecent of County Population",
    y = "Frequency",
    caption = "Source: CDC/ATSDR Social Vulnerability Index (SVI)\nhttps://www.atsdr.cdc.gov/placeandhealth/svi/index.html"
  ) +
  theme_minimal()

```

Now we will build some candidate models for our final predictive model for relative county-level turnout. This gives us an overview of how this feature set's relationship with the target response (```scaled_turnout```) are described by the underlying data. 

```{r more-model, echo=F, include=T, warning=F, error=F, fig.height=6, fig.width=6}

# Define two datasets for analysis:
all_years <- model_data

# Define a formula with all EPL variables
formula_ep <- scaled_turnout ~ EP_POV + EP_UNEMP + EP_NOHSDP + EP_AGE65 + EP_AGE17 + 
                          EP_SNGPNT + EP_LIMENG + EP_MINRTY + EP_MUNIT + 
                          EP_MOBILE + EP_CROWD + EP_NOVEH + EP_GROUPQ 

formula_epl <- scaled_turnout ~ EPL_POV + EPL_UNEMP + EPL_NOHSDP + EPL_AGE65 + EPL_AGE17 + 
                          EPL_SNGPNT + EPL_LIMENG + EPL_MINRTY + EPL_MUNIT + 
                          EPL_MOBILE + EPL_CROWD + EPL_NOVEH + EPL_GROUPQ 

# Fit OLS models for both datasets
model_all_years_ep <- lm(formula_ep, data = all_years)
model_all_years_epl <- lm(formula_epl, data = all_years)

# Summarize model results
summary_all_years_ep <- summary(model_all_years_ep)
summary_all_years_epl <- summary(model_all_years_epl)

# Print summaries to check coefficients and p-values
print(summary_all_years_ep)
print(summary_all_years_epl)

# Extract coefficients and p-values for visualization from the model with all years
coef_summary_epl <- tidy(model_all_years_epl) %>%
  filter(term != "(Intercept)") %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    )
  ) %>%
  mutate(type = "Using percentile ranked features")

coef_summary_ep <- tidy(model_all_years_ep) %>%
  filter(term != "(Intercept)") %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    )
  ) %>%
  mutate(type = "Using estimated proportion features")

coef_summary_both <- coef_summary_epl %>%
  union_all(coef_summary_ep) %>%
  arrange(p.value) %>%
  mutate(significance = fct_inorder(significance)) %>%
  arrange(type, estimate) %>%
  mutate(term = fct_inorder(str_remove(term, "^(EP_|EPL_)")))

# Plot coefficients
coef_summary_both %>%
ggplot(aes(x = term, y = estimate, fill = type)) +
  geom_bar(aes(fill = type), color="gray20", position=position_dodge2(width=0.9), stat = "identity") +
  coord_flip() +
  labs(
    title = "Coefficient Plot for SVI Features",
    x = "SVI Feature",
    y = "Coefficient Estimate",
    fill = "Data Used",
    caption = "Statistical significance codes:\n* = p-value < .01\n** = p-value < .001\n*** = p-value < super tiny number"
  ) +
  facet_wrap(type~., nrow=1, scales="free_x") + 
  scale_fill_manual(values = c("pink", "lightblue")) +
  geom_text(aes(label = significance), position=position_dodge2(width=0.9), hjust = -0.2, vjust = .7) +
  theme_minimal() +
    theme(legend.position = "bottom") 

```

Out of the gate, our basic vanilla linear models explain the data quite well, with an adjusted R-squared hovering around 40% and with a feature space filled with highly-significant variables, for both the model based on EP_ (proportion estimates) variables as well as the model based on EPL_ (percentile ranked) variables in the SVI. So, that's great news!

Given that we are getting similar predictive performance from both the EP_ and EPL_ series variables, I've chosen to proceed with the EP_ variables because the coefficients for our model will have a more intuitive, commonsense interpretation than with the EPL_ variables. The coefficients corresponding to EP_ variables allow us to make statements like "For every 1% increase in the estimated proportion of X, we can expect a Y% increase/decrease in turnout for that county." You know, for the sake of model explainability and all!

### Evaluating Multicollinearity in the Feature Set

We might imagine there is substantial multicollinearity in the feature space, so we should be aware of any strong correlations between our 13 features going into the modeling project. As a matter of good model design practice, let's take a peek at a correlation matrix for our SVI features. We'll see nothing here that doesn't make a lot of sense.

```{r corplot1,  echo=F, include=T, warning=F, error=F, fig.height=6, fig.width=6}

# Select only the predictor columns for correlation analysis
ep_data <- model_data %>%
  select(EP_POV, EP_UNEMP, EP_NOHSDP, EP_AGE65, EP_AGE17, 
         EP_SNGPNT, EP_LIMENG, EP_MINRTY, EP_MUNIT, 
         EP_MOBILE, EP_CROWD, EP_NOVEH, EP_GROUPQ, RPL_THEMES)

# Calculate the correlation matrix
corr_matrix_ep <- cor(ep_data, use = "complete.obs")

ggcorrplot(corr_matrix_ep, 
           method = "circle", 
           type = "lower", 
           lab = TRUE, 
           title = "Correlation Matrix of EP Variables",
           lab_size = 2, 
           colors = c("blue", "white", "red"))



```

### Motivating Regularization

Here, we see some obvious correlation between several of these features. One obvious example to call out is the moderate negative correlation between EP_AGE17 and EP_AGE65. Our hope is that we can rely on regularization to mute (or deselect entirely) some of these features where multicollinearity between certain features can inject bias into our predictive model.

Using R's ```glmnet``` package, we can use regularization to arrive at a more parsimonious (i.e., potentially fewer features) model with at least similar predictive performance to the basic OLS models we fitted to SVI variables earlier. This approach will *also* give us the ability to consider any number of interactions between these features, where interactions with a less-than-impactful contribution to predicting the response will be "penalized" out through the regularization process.

As mentioned above approach will also have the benefit of hopefully removing (via regularization penalization) some of the SVI features where there is excessive multicollinearity (if any). Using ```glmnet```, I've fixed the alpha parameter to 1, forcing a lasso regularization regime with L2 distances penalization, which is known to be better at "culling" the feature space to arrive at a more parsimonious model with fewer features).

### Refining the Feature Space

Before fitting the regularized model, several refinements to the feature set are warranted:

**Removing EP_POV**: The poverty variable (EP_POV) changed its definition from 100% of the federal poverty level (in SVI 2010-2018) to 150% of the poverty level (in SVI 2020+). This introduces a structural break in the feature across the training data, making it unreliable as a predictor. It is excluded from the final model.

**Log-transforming skewed variables**: Several EP_ variables (EP_LIMENG, EP_CROWD, EP_MUNIT) exhibit strong right-skew, with many counties clustered near zero. Applying a ```log(1 + x)``` transformation improves linearity with the response.

**Adding log(population)**: County total population (```E_TOTPOP```, already present in the SVI data) serves as a proxy for county scale and urbanicity. We include ```log(E_TOTPOP)``` to capture the well-known relationship between county size and turnout patterns.

**Adding election year**: Including the election year as a numeric feature allows the model to capture secular shifts in the relationship between SVI variables and turnout over time. Lasso regularization will zero this out if it is not informative.

**Weighting by county size**: Counties vary enormously in population, and ACS estimates for small counties have substantially higher standard errors. Weighting observations by ```log(VEP)``` in the loss function allows the model to focus on counties where the signal-to-noise ratio is highest.

**Leave-one-year-out cross-validation**: Rather than random 10-fold CV (which mixes counties from different election years in each fold), we use leave-one-year-out CV. This trains on two election cycles and tests on the third, directly measuring whether the model generalizes across elections — the actual use case.

### Lasso Model Results

Using the refined feature matrix (with all pairwise interactions), below shows a chart with the regressor values for primary variables and interactions that emerged from the ```glmnet``` process (where features where the absolute value of the regressor term is greater than .0005, as well as the intercept, are both excluded).


```{r turnout-model-regularization, echo=F, include=T, warning=F, error=F}

# Add derived features to model_data
model_data <- model_data %>%
  mutate(log_pop = log(E_TOTPOP),
         log_EP_LIMENG = log1p(EP_LIMENG),
         log_EP_CROWD = log1p(EP_CROWD),
         log_EP_MUNIT = log1p(EP_MUNIT))

# Build predictor matrix: EP_POV removed (definition changed in 2020),
# log-transforms applied to skewed variables, log(pop) and year added
predictors_ep <- model_data %>%
  select(EP_NOHSDP, EP_AGE65, EP_AGE17, EP_SNGPNT, EP_UNEMP,
         log_EP_LIMENG, EP_MINRTY, log_EP_MUNIT, EP_MOBILE, log_EP_CROWD,
         EP_NOVEH, EP_GROUPQ, log_pop, year) %>%
  model.matrix(~ .^2 - 1, .)

response <- model_data$scaled_turnout
obs_weights <- log(model_data$VEP_denom)

x_matrix_ep <- predictors_ep
y_vector <- response

set.seed(8675309)  # Set seed for reproducibility

# Leave-one-year-out cross-validation folds
fold_indices <- list(
  fold_2012 = which(model_data$year != 2012),
  fold_2016 = which(model_data$year != 2016),
  fold_2020 = which(model_data$year != 2020)
)

# Adjust the lambda grid to be fine-grained and range from small to large values
lambda_grid <- 10^seq(-6, 0, length = 100)
alpha_grid <- seq(.8, 1, length = 10)
tune_grid <- expand.grid(alpha = alpha_grid, lambda = lambda_grid)

# Define train control for leave-one-year-out cross-validation
train_control <- trainControl(
  method = "cv",
  index = fold_indices
)

# Run the train function with the custom grid, weighted by log(VEP)
model_ep <- train(
  x = x_matrix_ep,
  y = y_vector,
  weights = obs_weights,
  method = "glmnet",
  metric = "Rsquared",
  maximize = TRUE,
  trControl = train_control,
  tuneGrid = tune_grid
)

# Report the leave-one-year-out CV R-squared on scaled turnout
best_tune <- model_ep$results %>%
  filter(alpha == model_ep$bestTune$alpha, lambda == model_ep$bestTune$lambda)
print(paste("Leave-one-year-out CV R-squared (on scaled turnout):",
            round(best_tune$Rsquared, 4)))

# Extract coefficients from the best glmnet model
best_lambda <- model_ep$bestTune$lambda
coefficients <- coef(model_ep$finalModel, s = best_lambda)

# Convert to a data frame for easier plotting
coeff_df <- as.data.frame(as.matrix(coefficients))
colnames(coeff_df) <- "coefficient"
coeff_df$Feature <- rownames(coeff_df)
coeff_df <- coeff_df %>% filter(Feature != "(Intercept)") %>%
  filter(abs(coefficient) >= .0005)

# Plot coefficients using ggplot2
ggplot(coeff_df, aes(x = reorder(Feature, coefficient), y = coefficient)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Coefficient Plot for glmnet Model",
    subtitle = "Features with |coefficient| >= 0.0005",
    x = "Feature",
    y = "Coefficient Value"
  ) +
  theme_minimal()

```

# Testing the Model on 2024 Results

Now that we have a trained model, let's see how it performs on completely unseen data: the 2024 presidential election. The model was trained exclusively on 2012, 2016, and 2020 data, so 2024 represents a true out-of-sample validation.

```{r national-2024-data-prep, include=F, echo=F, warning=F, error=F}

# Prepare national 2024 model data using 2022 SVI
model_data_2024 <- svi_all %>%
  filter(RPL_THEMES >= 0) %>%
  filter(year %in% c(2022)) %>%
  mutate(year = year + 2) %>%
  inner_join(election_data_w_vep_no_out %>% select(GEOID, year, winning_party, VEP_denom, turnout_vep)) %>%
  group_by(year, ST_ABBR) %>%
  mutate(turnout_bystate_total = sum(turnout_vep*VEP_denom) / sum(VEP_denom)) %>%
  ungroup() %>%
  mutate(scaled_turnout = turnout_vep / turnout_bystate_total)

# Add derived features (matching training feature engineering exactly)
model_data_2024 <- model_data_2024 %>%
  mutate(log_pop = log(E_TOTPOP),
         log_EP_LIMENG = log1p(EP_LIMENG),
         log_EP_CROWD = log1p(EP_CROWD),
         log_EP_MUNIT = log1p(EP_MUNIT))

# Build prediction matrix (matching training matrix exactly)
predictors_ep_2024 <- model_data_2024 %>%
  select(EP_NOHSDP, EP_AGE65, EP_AGE17, EP_SNGPNT, EP_UNEMP,
         log_EP_LIMENG, EP_MINRTY, log_EP_MUNIT, EP_MOBILE, log_EP_CROWD,
         EP_NOVEH, EP_GROUPQ, log_pop, year) %>%
  model.matrix(~ .^2 - 1, .)

# Generate national predictions
predictions_2024 <- predict(model_ep, newdata = predictors_ep_2024)

# Build national analysis dataframe
analysis_2024 <- model_data_2024 %>%
  mutate(Predicted = predictions_2024,
         Actual = scaled_turnout) %>%
  mutate(Actual_Votes = turnout_vep * VEP_denom,
         Predicted_Votes = Predicted * turnout_bystate_total * VEP_denom)

```

## North Carolina: A Closer Look

First, let's zoom in on my home state of North Carolina, where the model performs quite well on the 2024 data.

```{r nc-vote, include=T, echo=F, warning=F, error=F}

# Filter to NC for the home-state spotlight
analysis_nc <- analysis_2024 %>%
  filter(ST_ABBR == "NC")

results_table_nc <- analysis_nc %>%
  select(LOCATION, Predicted_Votes, Actual_Votes) %>%
  mutate(Predicted_Votes = as.integer(Predicted_Votes)) %>%
  mutate(Absolute_Error_Pct = scales::label_percent(accuracy=.01)(abs(Actual_Votes - Predicted_Votes)/Actual_Votes))

kableExtra::kable(results_table_nc)

results_summary_nc <- results_table_nc %>%
  mutate(Absolute_Error_Pct = abs(Actual_Votes - Predicted_Votes)/Actual_Votes) %>%
  summarize(Predicted_Votes = sum(Predicted_Votes),
            Actual_Votes = sum(Actual_Votes),
            MAPE = scales::label_percent(accuracy=.01)(mean(Absolute_Error_Pct)))

print(results_summary_nc)

print(paste("NC R-squared (scaled turnout):", round(cor(analysis_nc$Actual, analysis_nc$Predicted)^2, 4)))

```

## National 2024 Validation

Having demonstrated the model's performance on North Carolina, let's now evaluate how the model performs across all US states (excluding Alaska) in the 2024 presidential election.

```{r national-2024-metrics, include=T, echo=F, warning=F, error=F}

# Overall National Metrics - R-squared on scaled turnout (the model response)
national_r2 <- cor(analysis_2024$Actual, analysis_2024$Predicted)^2

national_metrics <- analysis_2024 %>%
  mutate(abs_error = abs(Actual_Votes - Predicted_Votes),
         abs_pct_error = abs_error / Actual_Votes) %>%
  summarize(
    R_Squared = round(national_r2, 4),
    MAE = round(mean(abs_error), 0),
    MAPE = scales::label_percent(accuracy = 0.01)(mean(abs_pct_error)),
    Total_Predicted = as.integer(sum(Predicted_Votes)),
    Total_Actual = sum(Actual_Votes),
    N_Counties = n()
  )

print(paste("National R-squared (scaled turnout):", round(national_r2, 4)))
print(paste("National MAE:", round(national_metrics$MAE, 0), "votes"))
print(paste("National MAPE:", national_metrics$MAPE))
print(paste("Counties evaluated:", national_metrics$N_Counties))

```

```{r national-2024-scatter, include=T, echo=F, warning=F, error=F, fig.height=8, fig.width=10}

# Actual vs. Predicted Scatter Plot
analysis_2024 %>%
  ggplot(aes(x = Actual_Votes, y = Predicted_Votes)) +
  geom_point(aes(color = winning_party, size = VEP_denom), alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("DEMOCRAT" = "blue", "REPUBLICAN" = "red")) +
  scale_x_continuous(labels = scales::label_comma()) +
  scale_y_continuous(labels = scales::label_comma()) +
  labs(
    title = "Actual vs. Predicted County-Level Vote Counts (2024)",
    subtitle = paste0("R\u00b2 (scaled turnout) = ", round(national_r2, 4)),
    x = "Actual Total Votes",
    y = "Predicted Total Votes",
    color = "Winning Party"
  ) +
  guides(size = "none") +
  theme_bw() +
  theme(
    plot.title = element_text(size = 14),
    axis.title = element_text(size = 12)
  )

```

### State-Level Breakdown

```{r national-2024-state-breakdown, include=T, echo=F, warning=F, error=F}

# State-Level Breakdown
state_results <- analysis_2024 %>%
  mutate(abs_error = abs(Actual_Votes - Predicted_Votes),
         abs_pct_error = abs_error / Actual_Votes) %>%
  group_by(State = ST_ABBR) %>%
  summarize(
    N_Counties = n(),
    Total_Actual = sum(Actual_Votes),
    Total_Predicted = as.integer(sum(Predicted_Votes)),
    Total_Error_Pct = scales::label_percent(accuracy = 0.01)(
      abs(sum(Predicted_Votes) - sum(Actual_Votes)) / sum(Actual_Votes)),
    MAE = round(mean(abs_error), 0),
    MAPE = scales::label_percent(accuracy = 0.01)(mean(abs_pct_error)),
    R_Squared = round(cor(Actual, Predicted)^2, 4),
    .groups = "drop"
  ) %>%
  arrange(desc(Total_Actual))

kableExtra::kable(state_results, caption = "State-Level Prediction Performance (2024)")

```

```{r national-2024-state-error-chart, include=T, echo=F, warning=F, error=F, fig.height=10, fig.width=8}

# State-Level MAPE Bar Chart
state_mape <- analysis_2024 %>%
  mutate(abs_pct_error = abs(Actual_Votes - Predicted_Votes) / Actual_Votes) %>%
  group_by(State = ST_ABBR) %>%
  summarize(MAPE = mean(abs_pct_error), .groups = "drop") %>%
  arrange(MAPE)

state_mape %>%
  ggplot(aes(x = reorder(State, MAPE), y = MAPE)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  scale_y_continuous(labels = scales::label_percent(accuracy = 0.1)) +
  coord_flip() +
  labs(
    title = "Mean Absolute Percentage Error (MAPE) by State",
    subtitle = "2024 Presidential Election Predictions",
    x = "State",
    y = "MAPE"
  ) +
  theme_minimal()

```